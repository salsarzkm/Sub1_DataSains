# -*- coding: utf-8 -*-
"""Model_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17byzCBlSNGjKpZ1HMvtj9TdgVWXwu19w

# Proyek Belajar Data Sains: [HRD Problem]
- **Nama:** [Salsabila Rizka Maulidina]
- **Email:** [a004xbm448@devacademy.id atau salsaajadehhh@gmail.com]
- **ID Dicoding:** [a004xbm448]

# 1. Preparation

## Prepare the required library
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math
import joblib

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve, precision_recall_curve, auc
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier

"""## Prepare the required dataset"""

# Load data
df = pd.read_csv("employee_data.csv")

# Display head of the dataset
df.head(10)

"""# 2. Data Understanding

### Missing Value
The purpose of checking for missing values is to identify and handle incomplete data points to prevent errors during analysis and modeling, and to ensure the reliability and accuracy of the results.
"""

pd.DataFrame({'Total Missing Value': df.isna().sum()})

"""there are 412 data that are empty, so they will be deleted."""

# Delete rows whose Attrition value is empty
df = df.dropna(subset=['Attrition'])

# Check again
print(df['Attrition'].isnull().sum())

"""It can be seen that this dataset has no empty values, so it is ready to be analyzed.

### Data Type
To understand the initial dataset, we can use the info() function from pandas. This function shows the amount of data, the type of each column, and the presence of empty values.
"""

# Check data type of the dataset
df.info()

"""From the results above, it can be seen that this dataset contains 1058 entries with 35 columns. There are 1 column with float64 data type, 26 column with int64 data type, and 8 columns with object data type. There are no missing values in these columns, which means this dataset is complete and ready to be analyzed.

### Feature engineering
Feature engineering aims to create new, relevant features from raw data to improve the performance of machine learning models. It involves transforming, combining, or extracting 1  information from existing data to make patterns more visible to the algorithms, ultimately leading to better accuracy and efficiency.

New column will be created here
- IncomePerYearAtCompany: Indicates annual earnings relative to length of employment with the company.
- YearsInRoleRatio: Measures how long an employee has been in the current role compared to total work experience.
- PromotionDelayRatio: Measures what proportion of working time has been spent since the last promotion
- YearsWithManagerRatio: Measures how long a person works with the same manager compared to the length of time he or she has been with the company.
"""

# Make new column
df['IncomePerYearAtCompany'] = (df['MonthlyIncome'] * 12) / (df['YearsAtCompany'] + 1)
df['YearsInRoleRatio'] = df['YearsInCurrentRole'] / (df['TotalWorkingYears'] + 1)
df['PromotionDelayRatio'] = df['YearsSinceLastPromotion'] / (df['TotalWorkingYears'] + 1)
df['YearsWithManagerRatio'] = df['YearsWithCurrManager'] / (df['YearsAtCompany'] + 1)

# Display the result
df.head()

"""### Statistic Descriptive
To understand the distribution of the dataset, we can check by using describe()
"""

# Check statistic descriptive
df.describe()

"""### Encoding
Encoding is the process of converting categorical data (like text or labels) into numerical representations that algorithms can understand and process effectively. This is essential because most machine learning models work with numbers.

#### Label Encoding (Only 2 Value)
"""

le = LabelEncoder()

# Label encoding
df['Gender'] = le.fit_transform(df['Gender'])
df['OverTime'] = le.fit_transform(df['OverTime'])
df['Over18'] = le.fit_transform(df['Over18'])

"""#### One-Hot Encoding (> 2 Value)
drop_first=True digunakan untuk menghindari masalah multikolinearitas, karena jika tidak di-drop, akan mendapatkan satu kolom untuk setiap kategori dan itu bisa menyebabkan redundansi informasi.
"""

# One-Hot Encoding
df = pd.get_dummies(df, columns=['BusinessTravel', 'Department', 'JobRole', 'EducationField', 'MaritalStatus'], drop_first=True)

# Check the result
df.info()

"""After none of them are of object type, then we continue to the next step

### Standarization
This prevents features with larger values from dominating those with smaller values in machine learning algorithms, leading to faster convergence and improved model performance.
"""

# Determine the column to be normalized
columns_to_std = ['Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate',
                        'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',
                        'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',
                        'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager',
                        'IncomePerYearAtCompany', 'YearsInRoleRatio', 'PromotionDelayRatio', 'YearsWithManagerRatio']

#  MinMaxScaler Initialization
scaler = StandardScaler()

# Apply Min-Max Scaling to the data
df[columns_to_std] = scaler.fit_transform(df[columns_to_std])

# Check the result
df.describe()

"""### Correlation analysis between features
Correlation analysis between features aims to identify and quantify the linear relationships between different variables in a dataset. It helps understand how changes in one feature are associated with changes in another
"""

# KDefine numeric column
num_col = ['Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate',
            'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',
            'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',
            'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager',
            'IncomePerYearAtCompany', 'YearsInRoleRatio', 'PromotionDelayRatio', 'YearsWithManagerRatio']

"""#### Descriptive Statistics and Visualization"""

num_cols = len(num_col)
ncols = 3
nrows = math.ceil(num_cols / ncols)

# Histogram visualization for numerical features
plt.figure(figsize=(20, 5 * nrows))
for i, col in enumerate(num_col, 1):
    plt.subplot(nrows, ncols, i)
    sns.histplot(df[col], kde=True, bins=20)
    plt.title(f'Distribution of {col}', fontsize=12)
    plt.xlabel(col, fontsize=10)
    plt.ylabel('Frequency', fontsize=10)

plt.tight_layout(pad=4.0)

# Save the plot before showing
plt.savefig('descriptive statistics.png', dpi=300, bbox_inches='tight')

plt.show()

# Box plot for check outliers
plt.figure(figsize=(20, 5 * nrows))
for i, col in enumerate(num_col, 1):
    plt.subplot(nrows, ncols, i)
    sns.boxplot(x=df[col], color="skyblue", fliersize=5, linewidth=1.25)
    plt.title(f'Boxplot of {col}', fontsize=14)
    plt.xlabel(col, fontsize=12)
    plt.ylabel('Value', fontsize=12)

    # Customize the appearance of ticks and labels
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)

plt.tight_layout(pad=5.0)

# Save the plot before showing
plt.savefig('boxplots.png', dpi=300, bbox_inches='tight')

plt.show()

"""#### Correlation Analysis Between Features"""

# Calculating Pearson correlation between numerical features
correlation_matrix = df[num_col].corr()

# Visualization of correlations using heatmaps
plt.figure(figsize=(15, 12))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', cbar=True,
            linewidths=0.5, linecolor='gray', annot_kws={'size': 10},
            square=True, vmin=-1, vmax=1)

# Adjusting axis labels for readability
plt.title('Correlation Matrix between Numerical Features', fontsize=16)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(rotation=0, fontsize=12)

plt.tight_layout()

# Save the plot before showing
plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')

plt.show()

"""#### Correlation analysis with target
- A positive value indicates a positive correlation, meaning that as the feature value increases, the likelihood of 'Attrition' (e.g. marked with 1) also tends to increase. The closer to 1, the stronger the positive correlation.
- A negative value indicates a negative correlation, meaning that as the feature value increases, the likelihood of 'Attrition' tends to decrease (or vice versa). The closer to -1, the stronger the negative correlation.
- Values close to 0 indicate a weak or non-existent correlation between the feature and 'Attrition'.
"""

# Correlation between numerical features and the target 'Attrition'
attrition_correlation = df[num_col + ['Attrition']].corr()

# Visualization of correlation between features and targets
plt.figure(figsize=(10, 6))
sns.heatmap(attrition_correlation[['Attrition']], annot=True, cmap='coolwarm', cbar=True)
plt.title('Correlation of Features with Attrition')

# Save the plot before showing
plt.savefig('Correlation of Features with Attrition.png', dpi=300, bbox_inches='tight')

plt.show()

"""It can be seen that:
- negative correlation:
    1. Age (-0.17): The older the employee is, the less likely he/she is to leave the company.
    2. MonthlyIncome (-0.16): Employees with higher monthly income are less likely to leave.
    3. TotalWorkingYears (-0.18): The longer an employee's total work experience, the less likely they are to leave.
    4. YearsAtCompany (-0.14): The longer the employee has worked at the current company, the less likely they are to leave.
    5. YearsInCurrentRole (-0.16): The longer the employee has been in the current role, the less likely they are to leave.
    6. YearsWithCurrManager (-0.16): The longer an employee works with their current manager, the less likely they are to leave.
- Positive Correlations:
    1. DistanceFromHome (0.078): The further away an employee's home is from the office, the slightly more likely they are to leave.
    2. NumCompaniesWorked (0.037): Employees who have worked at more previous companies are slightly more likely to leave.
    3. PromotionDelayRatio (0.022): A higher promotion delay ratio is slightly correlated with the likelihood of leaving.
       
- Weak correlation: Many features show a very weak correlation (close to 0) with 'Attrition'. This means that the features do not have a strong linear relationship with the employee's decision to leave or not. For example: **'DailyRate', 'HourlyRate', 'MonthlyRate', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'TrainingTimesLastYear', 'YearsSinceLastPromotion', 'IncomePerYearAtCompany', 'YearsInRoleRatio', 'YearsWithManagerRatio'.**

#### Correlation Analysis Categorical Feature
This is to visualize the attrition rate for each feature category and the various categorical features that have been converted into dummy (binary) variables.
"""

# Visualization of Attrition distribution based on categorical features
cat_col = ['Gender', 'OverTime',  'BusinessTravel_Travel_Frequently',
           'BusinessTravel_Travel_Rarely', 'Department_Research & Development',
           'Department_Sales', 'JobRole_Human Resources', 'JobRole_Laboratory Technician',
           'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director',
           'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative',
           'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical',
           'EducationField_Other', 'EducationField_Technical Degree', 'MaritalStatus_Married',
           'MaritalStatus_Single']

# Histogram visualization for categorical features
plt.figure(figsize=(10, len(cat_col) * 0.5))
mean_attrition = [df[df[col]==1]['Attrition'].mean() for col in cat_col]
sns.barplot(x=mean_attrition, y=cat_col, color='skyblue')
plt.xlabel('Attrition Rate')
plt.title('Attrition Rate by Dummy Variables')
plt.tight_layout()

# Save the plot before showing
plt.savefig('Attrition Rate by Dummy Variables.png', dpi=300, bbox_inches='tight')

plt.show()

"""We can interpret the attrition rate as follows:
- Highest Influence (High Attrition Rate):
    1. OverTime (Yes): Overtime contributes greatly to employee attrition.
    2. JobRole_Sales Representative: This role has the highest attrition rate.
    3. MaritalStatus_Single: Single employees are more likely to leave.
- Medium Influence (Medium Attrition Rate)
    1. BusinessTravel_Travel_Frequently: Frequent business travel increases attrition.
    2. Multiple job roles: JobRole_Laboratory Technician, JobRole_Sales Executive.
    3. Some education fields: EducationField_Marketing, EducationField_Medical, EducationField_Life Sciences, EducationField_Technical Degree.
- Low Influence (Low Attrition Rate):
    1. BusinessTravel_Travel_Rarely: Rarely business traveling is associated with low attrition.
    2. Department_Research & Development: Working in this department tends to have low attrition.
    3. Some job roles: JobRole_Human Resources, JobRole_Manager, JobRole_Manufacturing Director, JobRole_Research Director.
    4. MaritalStatus_Married: Married employees are less likely to leave.
    5. EducationField_Other: Education field “Other” has low attrition.
    6. Gender: there are no significant differences.

# Data Preprocessing

This code segment separates the dataset into features (independent variables) and the target variable ('Attrition') for subsequent analysis or modeling
"""

# Split features and targets
X = df.drop(columns=['Attrition'])
y = df['Attrition']

"""Then, assesse the distribution of different classes within the target variable y and displays their proportional representation."""

# Check class imbalance
y.value_counts(normalize=True)

"""The preceding code checks the distribution of the target variable 'Attrition', revealing a class imbalance where '0.0' (likely representing no attrition) significantly outweighs '1.0' (likely representing attrition)

Next,  divide the dataset into separate training and testing sets for model evaluation.
"""

# Split testing and training set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

"""To address class imbalance in the training data, the following code applies the SMOTE (Synthetic Minority Over-sampling Technique)"""

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

"""# Modeling

XGBoost, or Extreme Gradient Boosting, represents a cutting-edge approach to machine learning that has garnered widespread acclaim for its exceptional performance in tackling classification and regression problems. By leveraging an ensemble of decision trees, XGBoost constructs a powerful predictive model through an iterative process that focuses on minimising errors.

The following code trains an XGBoost classification model using previously resampled training data to address class imbalance.
"""

# Train the model with resampled data
model = XGBClassifier(random_state=42)
model.fit(X_train_smote, y_train_smote)

"""The following code segment focuses on generating predictions and adjusting classification outcomes based on a probability threshold."""

# Predict
y_proba = model.predict_proba(X_test)[:, 1]
threshold = 0.45
y_pred_adjusted = (y_proba > threshold).astype(int)

"""# Evaluation

next, we will focuse on evaluating the performance of a classification model by generating a classification report, a confusion matrix, and calculating the ROC AUC score.
"""

# Evaluation
print("Classification Report:\n", classification_report(y_test, y_pred_adjusted))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_adjusted))
print("ROC AUC Score:", roc_auc_score(y_test, y_proba))

"""The model performs significantly better at classifying class 0.0 than class 1.0. This is evident from the higher precision, recall, and F1-score for class 0.0, as well as the confusion matrix showing fewer errors for class 0.0. The ROC AUC score of 0.802 indicates a good overall ability of the model to distinguish between the two classes, even though the performance on the minority class (1.0) is weaker. The weighted average in the classification report gives a more realistic overall performance considering the class imbalance.

The subsequent code segment focuses on visualizing the performance of a binary classification model through the generation of a Receiver Operating Characteristic (ROC) curve and a Precision-Recall (PR) curve, along with calculating the Area Under the ROC Curve (AUC).
"""

# ROC Curve
fpr, tpr, thresholds_roc = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(12, 5))

# ROC Curve
plt.subplot(1, 2, 1)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='blue')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()

# Precision-Recall Curve
precision, recall, thresholds_pr = precision_recall_curve(y_test, y_proba)
plt.subplot(1, 2, 2)
plt.plot(recall, precision, color='green')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')

plt.tight_layout()
plt.show()

"""For the ROC Curve, The blue curve is significantly above the random guess line, indicating that the model performs better than chance in distinguishing between the positive and negative classes. The AUC of 0.80 confirms this good performance. In the Precision-Recall Curve, The green curve shows the relationship between precision and recall for this model. To assess its performance, you would typically look at the average precision (the area under the PR curve) or examine the precision at different levels of recall. Without the average precision value displayed, we can observe the trade-offs: the precision is high at low recall values but generally decreases as the model tries to capture more of the positive instances (higher recall). The "jagged" nature of the curve is common."""

